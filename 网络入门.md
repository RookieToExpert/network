## 网络重要组件
1. 交换机：
    - 使相同网段下不同主机之间能通信
    - 交换机接口主要是两种：
        - access：连接终端，电脑，打印机
        - trunk：连接其他交换机

2. 网关(大部分路由器都是包含网关)：
    - 用来连接不同的网段
    - 云计算中，可以相同子网下的不同网段之间的通信，是通过网关/路由器
    - 网关会通过路由表(routing-table)去判断包应该去哪一个网段

    > 比如一个网关可能连接很多个网段

    ![alt text](/image/image.png)

    > 通过iptable实现

    ![alt text](image.png)

3. CDN：
    - 让不同的人访问相同域名时，返回不同的IP地址
## 协议区别
1. TCP：
    - 适合传文件，完整性要求高，对延迟不敏感，对每个包做确认处理，三次握手
    - 比如电子邮件，或者更新游戏

2. UDP：
    - 速度快，不会确认是否到达，允许丢包

## 网络安全
1. Vlan(虚拟局域网)：
    - 通过网络隔离解决网络隐患的覆盖率。
    - 交换机中创建Vlan，并将对应接口放入vlan中
    > 相同网段的IP如果在不同vlan，默认也是不通的
    
    ![alt text](image-1.png)
2. acl：
    - 创建访问控制规则
    - 调用这个规则



Sure. Our team was responsible for technical support in Microsoft Cloud migration, disaster recovery, and backup. We handled many related migration cases, which are highly relevant to the responsibilities of the current Alibaba Cloud architect role.

One case that left a strong impression on me was during the early period when we first started working with Microsoft Cloud Premium customers. The client was a large manufacturing enterprise in Australia. At that time, the case was still at the initial stage—the architect team was confirming the customer’s current architecture, business needs, and cloud adoption goals. The customer’s environment was dual-stack virtualization with nearly 300 VMs across both VMware and Hyper-V.

Our role was to engage with the Microsoft architect team and the customer’s virtualization, network, and operations teams, providing technical consulting and solutions around Microsoft migration products. At the start, we worked with the architect team to align on migration principles, such as the customer’s overall architecture, the timeline for test/production migration, fallback strategies, data security and compliance requirements, post-migration optimization, and specific RTO/RPO targets.

Pre-migration Assessment

Before migration, we assisted the customer in using the Microsoft Migrate assessment tools to establish a baseline of their current environment. By deploying the Azure Migration Appliance on-premises, we collected VM data including CPU utilization, disk I/O, and more. Based on P95 performance samples and safety margins agreed with the customer, we generated recommended Azure VM SKUs.

The assessment also included an Azure Readiness checklist for the on-prem environment—essentially a remediation plan to ensure workloads could be supported in Azure. This highlighted unsupported OS versions, disk size limits, and other constraints requiring changes before migration. A dependency map was also generated, which guided the migration in waves: first stateless workloads and those replaceable by PaaS (web servers, static sites, jump hosts), followed by middleware and business APIs, and finally the core databases.

Our target solution used ExpressRoute plus private endpoints, ensuring the data plane used private replication while the control plane minimized exposure to only essential public endpoints (e.g., 443).

Landing Zone & Replication Setup

The landing zone setup was owned by the architects, while we supported from the sidelines. They used Azure Front Door—an L7 service with CDN and firewall—to unify traffic entry for both on-prem and cloud workloads, supporting later dual-active scenarios and providing blue-green or canary strategies for cutover.

For replication, we helped deploy ASR/Migration Appliances on-premises. Microsoft supports agentless migration via VMware CBT and Hyper-V log tracking, performing full backups initially and then incremental replication for consistency. Replication cycles were based on half the time of the last incremental backup, with a minimum of 1 hour and a maximum of 12 hours.

Before production migration, we ran test migrations to validate VM creation and allow the customer to verify basic functions—SSH/RDP login with domain accounts, disk attachments, dependency ports, API calls, and DNS resolution. In production migration, we performed a final sync, shut down source workloads to prevent data inconsistency, and executed cutover with a downtime window of ~20 minutes.

Key Challenges & Solutions

Of course, the process was not as smooth as I just described. We encountered many issues—let me highlight a couple of the most memorable ones.

Hardcoded IP dependencies: Some legacy applications had IP-based communication hardcoded. Since the cloud and on-prem networks couldn’t share overlapping IP ranges, migrating changed the IPs and caused failures. We recommended replacing IPs with FQDNs, but the customer was unwilling to make large-scale changes in production. As a workaround, we configured DNAT/SNAT rules on the on-prem firewall to map old IPs to new cloud IPs, adjusting routing so the firewall became the next hop. This ensured the applications continued functioning post-migration. Though only a temporary solution, we strongly advised the customer to migrate to FQDN in the future.

Typical challenges and mitigations:

Private DNS resolution – Initially the customer pointed forwarders to 168.63.129.16, breaking Private Endpoint resolution. As a quick fix we used hosts files, then implemented Private DNS Zones with conditional forwarding.

IP changes and coupling – To decouple dependencies, we reduced DNS TTLs, promoted FQDN over hardcoding, and mapped authoritative CNAMEs to Front Door. Where refactoring costs were too high, we used DNAT/SNAT or reverse proxies to maintain return path symmetry.

High churn and bandwidth limits – We enabled high-churn mode, switched cache to Premium Block Blob, distributed load across multiple storage accounts, scaled out appliances, and throttled replication by workload and time of day. Extremely high-write disks were isolated for special handling.

Outcomes & Methodology

Many stateless workloads were re-platformed to PaaS, reducing server count by half.

Right-sizing, elasticity, and tiered storage significantly optimized TCO.

ExpressRoute + Private Endpoints + least privilege + full auditing ensured compliance.

Wave-based migration, rehearsals, and rollback strategies enabled controlled cutovers with zero rollback incidents.

From a TOGAF perspective, the project exemplified a structured methodology: Business alignment → Baseline assessment → Target architecture → Opportunities & solutions → Migration planning → Governance. Key principles were: governance before migration (accounts, networks, DNS, permissions), rehearsals before cutover, re-platform when possible, minimize control plane exposure, and prioritize data-plane security.

In the end, we achieved the goal: workloads migrated successfully, ran stably, were well-governed, and at optimized cost.

The client was a Microsoft Unified Premier customer—a large Australian manufacturing firm with dual vSphere/Hyper-V clusters hosting 300+ VMs across web, app, middleware, and database layers (Oracle, PostgreSQL, Redis, logging servers, Python APIs, etc.). They demanded very short cutover windows, so we worked closely with their virtualization, networking, and operations teams alongside Microsoft architects to deliver this solution.